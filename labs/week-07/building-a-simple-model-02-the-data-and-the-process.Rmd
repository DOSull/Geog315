#### Geog 315 T2 2021 - materials still to be finalised for T1 2022

**You may find that the easiest way to follow this document is by knitting it to a HTML file using the Knit button. To run the various commands discussed you should work in _RStudio_.**

# Building a simple statistical model
In this assignment you will build a simple regression model of the Airbnb listings in and around Wellington that we assembled a few weeks ago. The model will aim to account for variation in the numbers of listings with respect to the age structure of the population (from census) and relative to the numbers of various 'amenities' such as caf√©s, retail, and so on.

## Libraries
Before you start, as usual we need some libraries.
Next, load the libraries we'll be using.

```{r}
library(sf)
library(tmap)
library(dplyr)
library(magrittr)
tmap_mode("view")
```

## The data
Provided you have unpacked this week's materials to an accessible folder you should find the datasets by simply running the commands shown. If that doesn't seem to work, then download the data from the links provided in the section below. You should find all the data in a subfolder called `data`, if you unpacked the zip file correctly. 

### Base data
The base data are in [this file](data/wellington-base-data.gpkg?raw=true). Open them and take a look.

```{r}
welly <- st_read("data/wellington-base-data.gpkg")
plot(welly, lwd = 0.5)
```

Remember that the high values in this colour scheme are at the yellow end of the colour ramp. If you want to get a better feel for the distribution of different variables, then I recommend you make some `tmap` maps.

The variables in the base data are

+ `sa2_id` and `sa2_name` are the Statistical Area 2 identifier and name respectively.
+ `ta_name` is the Territorial Authority name, limited to just Wellington City and Lower Hutt City (this is a smaller area than we originally looked at, to allow slightly easier mapping).
+ `pop` is the total population of each Statistical Area 2 from the 2018 Census.
+ `u15`, `a15_29`, `a30_64`, `o65` are the percentages of the population in the 2018 census in the age groups Under 15, 15 to 29, 30 to 64 and 65 and over, respectively.
+ `dist_to_centre` distance in km to a location somewhere in the centre of the city. This is an approximation only, and is measured in a straight line, not over the road network.

### Airbnb locations
We already saw [this dataset](data/abb.gpkg?raw=true) recording all the Airbnb locations across the wider region.

```{r}
abb <- st_read("data/abb.gpkg")
tm_shape(abb) + 
  tm_dots()
```
### Amenities
This [dataset](data/amenities.gpkg?raw=true) has the locations of a range of services, as recorded in [OpenStreetMap](https://www.openstreetmap.org) (OSM). 

```{r}
amenities <- st_read("data/amenities.gpkg")
tm_shape(amenities) +
  tm_dots(col = "amenity")
```

The type of amenity is stored in the `amenity` attribute. This is a reduced set of all the available amenities in OSM because the raw data includes things like bike stands and a wide array of things that only show up once or maybe twice across the region.

### Retail
Finally we have [data on shops](data/shops.gpkg?raw=true). 

```{r}
shops <- st_read("data/shops.gpkg")
tm_shape(shops) + 
  tm_dots(col = "shop")
```

In this dataset the type of shop is recorded in the `shop` attribute. YOu'll see there are a wide array of types of shop including some surprising classifications such as military_surplus.

### **Question 1**
#### Based on making and exploring some maps, which variables in the `welly` base dataset, and which point locations in the `amenities` and `shops` datasets do you think are most likely to help account for the number of Airbnb listings across the region? There is no strictly correct answer to this question. The idea is to think about what you are doing before just making a statistical model without thinking!. (**20%**)

You will probably find it helpful to make other maps using `tmap` to answer this question.

## Organising the data
The idea is to account for the numbers of Airbnb listings per Statistical Area 2 (SA2) using attributes in the base data `welly` and in the two point datasets `amenities` and `shops`.

To do this we need to count the points in the SA2 polygons. We saw how to do this [a few weeks ago](https://dosull.github.io/Geog315/labs/week-04/spatial-data-manipulation-04-counting-points-in-polygons.html), but as a reminder, here's how to count the Airbnb listings

```{r}
n_abb <- welly %>%
  st_contains(abb) %>%
  lengths()
n_abb
```

And this list of counts can then be added to the base data using a simple `mutate` operation

```{r}
welly %<>%
  mutate(n_abb = n_abb)
```

And then we can map this:

```{r}
tm_shape(welly) + 
  tm_polygons(col = "n_abb")
```

Similar operations can be applied to count the amenities and/or shops points.

When you make the model requested for this assignment, an important step is to decide which amenities and which shops in the respective datasets matter. You could include some, none or all of them each represented as a count of how many there are of the respective types in each SA2. Adding these to the base dataset will involved filtering the point data down to only the types you want to keep, and then counting them using an operation like the above, then adding them into the base data.

When you make these additions to the dataset, I recommend that you save it to a new file so you don't lose the work, using the `st_write()` function. 

## Building a regression model
Because *R* is a platform for statistical computing, making regression models (or more generally linear models) is central to what it does. We're going to make a model using all the census variables as independent variables to fit the `n_abb` variable which represents the numbers of Airbnb listings. This is actually a bad idea for reasons we get to a bit later.

The `lm` function does all the work, all it needs is the equation we want to fit, which we specify as shown below:

```{r}
m.ages <- lm(n_abb ~ u15 + a15_29 + a30_64 + o65, data = welly)
```

Nothing seems to have happened, but a model has been made, and we can see what it looks like with the `summary` function:

```{r}
summary(m.ages)
```

The *** designations tell us which of the independent variables are most statistically significant, in this case, it seems like `u15` has that honour, but two of the other age groups variables are also important.

The sign of the coefficients in the `Estimate` column tells us the sense of the relationship: positive signs mean that when that attribute increases the dependent variable also increases, while negative signs mean the opposite: where that attribute is higher the dependent variable tends to be lower. The values of the coefficients also tell us how much change to expect in the dependent variable for each unit change in the independent variable. For example, for every percentage point increase in the population under 15 population the model is saying that we expect about 0.8 fewer listings in a census tract.

### **Question 2**
#### Write a brief interpretation of this model describing in words what it seems to tell us about the effect of different age group percentages on the numbers of Airbnb listings in neighbourhoods. (**20%**)

## Residual mapping
*Residuals* are the model errors - the variation in the dependent variable that the model does not account for. Mapping residuals can be informative. Model residuals are available from the model variable we made `m.ages`, and can be added to the spatial data as a new attribute to be mapped:

```{r}
welly %<>%
  mutate(residual = m.ages$residuals)

tm_shape(welly) +
  tm_polygons(col = 'residual', palette = 'RdBu', alpha = 0.65)
```

Where the residuals are high (i.e., positive), the model is *underestimating* the number of listings, that is there are _more_ listings than we might reasonably expect. Where the residuals are low (i.e., negative) the model is *overestimating* the number of listings, that is there are fewer listings than might be expected. You might find it easier to store the residual as `-m.ages$residuals` and call it `m.ages.error` so that a negative value means the model is _underestimating_ and a positive value means it is _overestimating_. 

Across most of the area, the model is not doing terribly but there are a couple of places where it is badly out. The next question asks you to examine these places a bit more closely, using the web map view, for contextual information that might explain the weakness of the model in these places. The best web map for that contextual information is probably the OpenStreetMap layer.

### **Question 3**
#### Where does the model do particularly badly? Briefly speculate on what other factors missing from this particular model (which uses only age data) might explain this. (**25%**)

## Challenges and interpretations
One of the many difficulties with this model (which wasn't made with too much thought) is the problem of _multicollinearity_ which means that related variables can mask one another out in a regression model. Here because the different age group percentages must sum to 100, they are not independent of one another. This can lead to instability in model coefficients. For example, if we remove one of the age groups, unexpected changes happen in the model. To see this here's a model that leaves out the percentage aged under 15 variable.

```{r}
m.no_children <- lm(n_abb ~ a15_29 + a30_64 + o65, data = welly)
summary(m.no_children)
```

This model implies that every percentage point increase in any of the three age groups included will increase the number of Airbnb listings, and also that the 15-29 year old age group has the largest effect, where before the effect of the 30 to 64 year old age group was greater.

This demonstrates how complicated the interrelationships among many attributes in a dataset can be, and how if we control for one factor, it can change the apparent effect of other factors.

## Finally: build your own model
With the data available, and based on all you have seen so far, make your own model. 

You should consider including any of the other variables provided such as total population (`pop`), the distance to centre, and (after filtering and counting them) any of the various amenity types or shop types.

You can also experiment with the `step()` function discussed in lecture [here](https://southosullivan.com/geog315/video/week-07-lecture/regression-10.mp4).

When you have made a model you are happy with answer the following question.

### **Question 4**
#### For the model you made, include the code used to generate it and the output from the `summary` function. Also make a residuals map of your model. Briefly explain why you chose to build the model you did. What influenced your choice of variables to include? Explain what your model seems to show based on the results provided by the `summary` function. (**35%**)
